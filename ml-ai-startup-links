https://www.predictionmachines.ai/
https://pubsonline.informs.org/doi/10.1287/orsc.11.4.448.14602
https://www.wired.com/story/ai-pioneer-explains-evolution-neural-networks/
https://hbr.org/2016/12/the-scary-truth-about-corporate-survival
https://edition.cnn.com/interactive/2018/10/business/amazon-history-timeline/index.html
https://www.visualcapitalist.com/how-long-does-it-take-to-hit-50-million-users/
https://sloanreview.mit.edu/article/an-interview-with-clayton-m-christensen/
https://claytonchristensen.com/key-concepts/
https://www.brunswickgroup.com/interview-walter-isaacson-i10012/
https://www.shortform.com/C/summary-v2/lean-startup-summary-eric-ries?gclid=Cj0KCQjww_f2BRC-ARIsAP3zarH3_wFTCtgDdZotIyhmkKFZkEg7Z_pOSZ6EXNWCud5IFJkq2ii-5e4aAnGYEALw_wcB
https://www.amazon.com/Innovators-Hackers-Geniuses-Created-Revolution/dp/1476708703/ref=sr_1_1?crid=1T1TD7VNLPOCM&keywords=innovators+isaacson&qid=1558367082&s=gateway&sprefix=innovators+isaa%2Caps%2C114&sr=8-1
https://media.techhippo.org/wp-content/uploads/2020/07/30211425/Schermata-2020-07-30-alle-21.13.55-1024x208.png

Perceptron functions:
Sigmoidfunktion https://de.wikipedia.org/wiki/Sigmoidfunktion
ReLu https://de.wikipedia.org/wiki/Rectifier_(neuronale_Netzwerke)
Softplus https://medium.com/@abhinavr8/activation-functions-neural-networks-66220238e1ff
Swish https://en.wikipedia.org/wiki/Swish_function

https://en.wikipedia.org/wiki/Hyperparameter_optimization

Chart on neural networks
https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464

Important people
https://en.wikipedia.org/wiki/John_von_Neumann

https://en.m.wikipedia.org/wiki/Catherine_Tucker (AI Ethics Professor)

###Ethics & Economics of AI###
https://media.techhippo.org/wp-content/uploads/2020/11/12114050/Awadetal_MoralMachineExperiment.pdf
https://media.techhippo.org/wp-content/uploads/2020/11/12114042/AutorLevyMurnane_TheSkillContentOfRecentTechnologicalChange.pdf
https://media.techhippo.org/wp-content/uploads/2020/11/12114051/BonnefonShariffRahwan_SocialDilemmaAutonomousVehicles.pdf
Prediction vs Causality: general problem of spurious correlations that falsely seem to imply causal link between observations

###Two biases of predictions (Prosperi et al., Nature Machine Intelligence, 2020):
Confounding bias: common cause for both exposure/predictor and outcome. AI would detect that lung cancer comes along with yellow fingers that both result from smoking, but getting normal skin color back does not heal cancer.
Collider bias: common effect of both exposure and outcome. Hospitalization status functions as a collider because it introduces selection bias, as people with locomotor disease or respiratory disease have a higher risk of being admitted to a hospital, but treating one will not heal the other.

###NLP###
natural language processing (NLP). NLP follows the steps below:

Tokenization: splits longer strings of text into smaller pieces, or tokens; larger chunks of text can be tokenized into sentences, sentences can be tokenized into words, etc.
Normalization: 
Word Stemming: eliminating affixes (suffixed, prefixes, infixes, circumfixes) from a word to obtain a word stem (infinitive, singular…)
Lemmatization: capture canonical forms based on a word’s lemma (e.g. better → good)
Set all characters to lowercase
Convert numbers to textual representations or vice versa
Remove punctuation 
Remove default stop words (articles, conjunctions etc.)
BERT – Bidirectional Encoder Representations from Transformers

Created and published in 2018 by Jacob Devlin and his colleagues from Google
Original goal: better understand user searches
24-layer, 1024-hidden, 16-heads, 340M parameter neural network architecture
Trained on BookCorpus with 800M words and English Wikipedia with 2,500M words
Deeply bidirectional, unsupervised language representation
Takes into account the context for each occurrence of a given word, contextual model that generates a representation of each word that is based on the other words in the sentence
Not possible to train bidirectional models by simply conditioning each word on its previous and next words, since this would allow the word that’s being predicted to indirectly “see itself” in a multi-layer model
Solution: mask words and predict them (“Masked Language Modeling”) 

###Business Modeling###
https://medium.com/epitome-of-innovation-and-technology-management/xerox-116185e565e1

###Business modeling###
https://www.youtube.com/watch?v=DOtCl5PU8F0&feature=youtu.be
##Key points##
Founders - 1 of 10
Market - +20%
Product - 10x
Acquistion - 0$
Monopoly - Boolean
